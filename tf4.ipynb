{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 代价函数\n",
    "\n",
    "## 1.1 二次代价函数\n",
    "把label减去预测值的误差的平方累加起来再求平均值\n",
    "$$C=\\frac1{2n}\\sum_x{\\lVert{y(x)-a^L(x)}\\rVert^2}$$\n",
    "其中，C表示代价函数，x表示样本，y表示实际值，a表示输出值，n表示样本的总数。为简单起见，以一个样本为例进行说明，此时二次代价函数为：\n",
    "$$C=\\frac{(y-a)^2}2$$\n",
    "* $a=\\sigma(z), z=\\sum{W_j*X_j}+b$\n",
    "* $\\sigma()$是激活函数\n",
    "\n",
    "假如我们使用梯度下降法(Gradient descent)来调整权值参数的大小，权值w和偏置b的梯度推导如下：\n",
    "$$\\frac{\\partial{C}}{\\partial{w}}=(a-y)\\sigma'(z)x$$\n",
    "\n",
    "$$\\frac{\\partial{C}}{\\partial{b}}=(a-y)\\sigma'(z)$$\n",
    "\n",
    "其中，z表示神经元的输入，σ表示激活函数。w和b的梯度跟激活函数的梯度成正比，激活函数的梯度越大，w和b的大小调整得越快，训练收敛得就越快。反之，如果激活函数的梯度几乎等于0，那么w和b几乎不会调整，这就是 **gradient vanish**。\n",
    "\n",
    "## 1.2 交叉熵代价函数\n",
    "\n",
    "假设我们的激活函数是sigmoid函数\n",
    "![](http://upload-images.jianshu.io/upload_images/1791718-29de59f2ebc8571b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "* 假设我们的目标是收敛到1，A点为0.82，离目标较B点更远，而A点处的梯度也比B点处大，权值调整比较大。B点为0.98，离目标较近，梯度比较小，权值调整也比较小，调整方案合理；\n",
    "* 假设我们的目标是收敛到0，A点为0.82，离目标较B点更近，而A点处的梯度比B点处大，权值调整比较大。B点为0.98，离目标较远，梯度却比较小，权值调整也比较小，调整方案不合理；\n",
    "\n",
    "换一个思路，我们不改变激活函数，而是改变代价函数，改用交叉熵代价函数：\n",
    "$$C=-\\frac1n\\sum_x{[ylna+(1-y)ln(1-a)]}$$\n",
    "其中，C表示代价函数，x表示样本，y表示实际值，a表示输出值，n表示样本的总数。\n",
    "* $a=\\sigma(z), z=\\sum{W_j*X_j}+b$\n",
    "* $\\sigma'(z)=\\sigma(z)(1-\\sigma(z))$\n",
    "\n",
    "![](http://upload-images.jianshu.io/upload_images/1791718-ee3d72911c4506d6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "* 权值和偏置值的调整与$\\sigma'(z)$无关，另外，梯度公式中的$\\sigma(z)-y$表示输出值与实际值的误差。所以当误差越大时，梯度就越大，参数w和b的调整就越快，训练的速度也就越快。这样是比较合理的。\n",
    "* 如果输出神经元是线性的，那么二次代价函数就是一种合适的选择。如果输出神经元是sigmoid函数，那么比较适合用交叉熵代价函数。\n",
    "\n",
    "## 1.3 对数似然代价函数\n",
    "\n",
    "对数似然函数常用来作为softmax回归的代价函数，如果输出层神经元是sigmoid函数，可以采用交叉熵代价函数。而深度学习中更普遍的做法是将softmax作为最后一层，此时常用的代价函数是对数似然函数。\n",
    "\n",
    "对数似然代价函数与softmax的组合和交叉熵与sigmoid函数的组合非常相似。对数似然代价函数在二分类时可以化简为交叉熵代价函数的形式。\n",
    "\n",
    "在TensorFlow中用：\n",
    "* `tf.nn.sigmoid_cross_entropy_with_logits()`来表示跟sigmoid搭配使用的交叉熵。\n",
    "* `tf.nn.softmax_cross_entropy_with_logits()`来表示跟softmax搭配使用的交叉熵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#载入数据集\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\",one_hot=True)\n",
    "\n",
    "#每个批次的大小\n",
    "batch_size = 100\n",
    "#计算一共有多少个批次\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "#定义两个placeholder\n",
    "x = tf.placeholder(tf.float32,[None,784])\n",
    "y = tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "#创建一个简单的神经网络\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "prediction = tf.nn.softmax(tf.matmul(x,W)+b)\n",
    "\n",
    "#二次代价函数\n",
    "# loss = tf.reduce_mean(tf.square(y-prediction))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n",
    "#使用梯度下降法\n",
    "train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "\n",
    "#初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#结果存放在一个布尔型列表中\n",
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax返回一维张量中最大的值所在的位置\n",
    "#求准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(21):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys =  mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys})\n",
    "        \n",
    "        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "        print(\"Iter \" + str(epoch) + \",Testing Accuracy \" + str(acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 过拟合\n",
    "\n",
    "防止过拟合\n",
    "* 增加数据集\n",
    "* 正则化\n",
    "* Dropout\n",
    "\n",
    "**Dropout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#载入数据集\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\",one_hot=True)\n",
    "\n",
    "#每个批次的大小\n",
    "batch_size = 100\n",
    "#计算一共有多少个批次\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "#定义两个placeholder\n",
    "x = tf.placeholder(tf.float32,[None,784])\n",
    "y = tf.placeholder(tf.float32,[None,10])\n",
    "keep_prob=tf.placeholder(tf.float32)\n",
    "\n",
    "#创建一个简单的神经网络\n",
    "\n",
    "# 2000个神经元的隐藏层\n",
    "W1 = tf.Variable(tf.truncated_normal([784,2000],stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([2000])+0.1)\n",
    "L1 = tf.nn.tanh(tf.matmul(x,W1)+b1)\n",
    "L1_drop = tf.nn.dropout(L1,keep_prob) \n",
    "\n",
    "# 2000个神经元的隐藏层\n",
    "W2 = tf.Variable(tf.truncated_normal([2000,2000],stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([2000])+0.1)\n",
    "L2 = tf.nn.tanh(tf.matmul(L1_drop,W2)+b2)\n",
    "L2_drop = tf.nn.dropout(L2,keep_prob) \n",
    "\n",
    "# 1000个神经元的隐藏层\n",
    "W3 = tf.Variable(tf.truncated_normal([2000,1000],stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([1000])+0.1)\n",
    "L3 = tf.nn.tanh(tf.matmul(L2_drop,W3)+b3)\n",
    "L3_drop = tf.nn.dropout(L3,keep_prob) \n",
    "\n",
    "# output\n",
    "W4 = tf.Variable(tf.truncated_normal([1000,10],stddev=0.1))\n",
    "b4 = tf.Variable(tf.zeros([10])+0.1)\n",
    "prediction = tf.nn.softmax(tf.matmul(L3_drop,W4)+b4)\n",
    "\n",
    "#二次代价函数\n",
    "# loss = tf.reduce_mean(tf.square(y-prediction))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n",
    "#使用梯度下降法\n",
    "train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "\n",
    "#初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#结果存放在一个布尔型列表中\n",
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax返回一维张量中最大的值所在的位置\n",
    "#求准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(31):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys =  mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys,keep_prob:0.7})\n",
    "        \n",
    "        test_acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0})\n",
    "        train_acc = sess.run(accuracy,feed_dict={x:mnist.train.images,y:mnist.train.labels,keep_prob:1.0})\n",
    "        print(\"Iter \" + str(epoch) + \",Testing Accuracy \" + str(test_acc) +\",Training Accuracy \" + str(train_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 优化器\n",
    "\n",
    "## 3.1 Optimizer\n",
    "```\n",
    "tf.train.GradientDescentOptimizer\n",
    "\n",
    "tf.train.AdadeltaOptimizer\n",
    "\n",
    "tf.train.AdagradOptimizer\n",
    "\n",
    "tf.train.AdagradDAOptimizer\n",
    "\n",
    "tf.train.MomentumOptimizer\n",
    "\n",
    "tf.train.AdamOptimizer\n",
    "\n",
    "tf.train.FtrlOptimizer\n",
    "\n",
    "tf.train.ProximalGradientDescentOptimizer\n",
    "\n",
    "tf.train.ProximalAdagradOptimizer\n",
    "\n",
    "tf.train.RMSPropOptimizer\n",
    "```\n",
    "\n",
    "## 3.2 各种优化器对比\n",
    "\n",
    "**标准梯度下降法：**\n",
    "\n",
    "标准梯度下降先计算所有样本汇总误差，然后根据总误差来更新权值\n",
    "\n",
    "**随机梯度下降法：**\n",
    "\n",
    "随机梯度下降随机抽取一个样本来计算误差，然后更新权值\n",
    "\n",
    "**批量梯度下降法：**\n",
    "\n",
    "批量梯度下降算是一种折中的方案，从总样本中选取一个批次（比如一共有10000个样本，随机选取100个样本作为一个batch），然后计算这个batch的总误差，根据总误差来更新权值。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "#载入数据集\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\",one_hot=True)\n",
    "\n",
    "#每个批次的大小\n",
    "batch_size = 100\n",
    "#计算一共有多少个批次\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "#定义两个placeholder\n",
    "x = tf.placeholder(tf.float32,[None,784])\n",
    "y = tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "#创建一个简单的神经网络\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "prediction = tf.nn.softmax(tf.matmul(x,W)+b)\n",
    "\n",
    "#二次代价函数\n",
    "# loss = tf.reduce_mean(tf.square(y-prediction))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n",
    "#使用Adam进行优化 学习率设置为0.01\n",
    "# train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "train_step = tf.train.AdamOptimizer(1e-2).minimize(loss)\n",
    "\n",
    "#初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#结果存放在一个布尔型列表中\n",
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax返回一维张量中最大的值所在的位置\n",
    "#求准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(21):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys =  mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys})\n",
    "        \n",
    "        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "        print(\"Iter \" + str(epoch) + \",Testing Accuracy \" + str(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
